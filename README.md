# de101â€‘dataâ€‘pipeline
## ðŸ“– Project Overview

`de101-data-pipeline` is a learning/proofâ€‘ofâ€‘concept project to build batch ETL pipelines for multiple data domains such as stock market data and game logs. The goal is to practice and demonstrate:

- **Orchestration**: Using Apache Airflow to schedule and run workflows  
- **Data Validation**: Applying Great Expectations to validate data after extraction  
- **Transformation**: Using dbt to perform further modeling and transformations  
- **Scalable Deployment**: Containerization with Docker and eventual deployment to Kubernetes via Helm  

This project is ideal for anyone looking to build handsâ€‘on experience in a full-fledged data engineering pipeline.

---

## ðŸ“‚ Directory Structure
```
de101-data-pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ project1/
â”‚   â”‚   â”œâ”€â”€ etl/               # Airflow DAGs for ETL tasks
â”‚   â”‚   â”œâ”€â”€ ge_checks/         # DAGs for Great Expectations validations
â”‚   â”‚   â””â”€â”€ dbt/models/        # dbt models for the project/domain
â”‚   â””â”€â”€ other_domain/          # Placeholder for future domains
â”œâ”€â”€ plugins/                    # Custom Airflow Operators, Hooks, and Sensors
â”œâ”€â”€ src/                        # ETL modules (extract / transform / load)
â”œâ”€â”€ configs/                     # Configuration files per environment
â”‚   â”œâ”€â”€ dev.yaml
â”‚   â””â”€â”€ prod.yaml
â”œâ”€â”€ docker/                      # Dockerfiles & docker-compose for local setup
â”‚   â”œâ”€â”€ Dockerfile.airflow
â”‚   â””â”€â”€ docker-compose.yaml
â”œâ”€â”€ helm/airflow/                # Helm chart for Kubernetes deployment
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ airflow-deployment.yaml
â”‚   â””â”€â”€ secrets/
â”‚       â””â”€â”€ airflow-connections.yaml
â”œâ”€â”€ tests/                       # Unit tests for DAGs, src modules, plugins
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```


---

## ðŸ›  Technology Stack
- **Orchestration**: Apache Airflow  
- **Validation**: Great Expectations  
- **Modeling / Transformation**: dbt  
- **Containerization**: Docker Compose  
- **Deployment (future)**: Kubernetes + Helm  
- **Storage / Targets**: S3 / MinIO, Databricks  

---

## ðŸš€ Getting Started

### Prerequisites

- Docker & Docker Compose  
- (Optional) Access credentials for S3 / MinIO or Databricks  
- Python 3.9+ (for local dev)  

### Local Setup

1. Clone the repository  
   ```bash
   git clone https://github.com/your-username/de101-data-pipeline.git
   cd de101-data-pipeline


Create a .env file based on the example (or config):

AWS_ACCESS_KEY=your_access_key
AWS_SECRET_KEY=your_secret_key
S3_ENDPOINT=http://localhost:9000


Build and start Airflow via Docker Compose:

docker-compose up --build


Open Airflow UI:
Navigate to http://localhost:8080 in your browser.

âœ… Example Workflow

Airflow triggers an ETL DAG (e.g. stock/etl/daily)

ETL job extracts raw data, transforms it, and loads it into a staging location (S3 or Databricks)

Once loaded, a GE validation DAG runs to check data quality

If validation succeeds, a dbt DAG runs to model data and produce final tables

ðŸ“¦ Configuration

All environment-specific configurations are located in configs/:

configs/dev.yaml â€” for local development

configs/staging.yaml â€” for staging environment

configs/prod.yaml â€” for production

These config files define connections (S3, Databricks), credentials, and other environment variables.

ðŸ‘¥ Contributing

Contributions are welcome! Here's how you can help:

Fork the repository

Create a new branch: git checkout -b feature/your-feature

Make your changes (add DAG, improve src, etc.)

Add / update tests under tests/

Commit your changes: git commit -m "feat: description of your change"

Push to your branch: git push origin feature/your-feature

Open a Pull Request

Please follow existing code style, and ensure new code is covered by tests.

ðŸ“„ License

This project is licensed under the MIT License. See the LICENSE file for details.

ðŸ”— Links & Resources

Airflow Documentation

Great Expectations Documentation

dbt Documentation